{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e102fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import multiprocessing\n",
    "#from multiprocessing import Pool\n",
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44179049",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "count = 0\n",
    "with open('/Users/vedant_j/Desktop/ucsd_assignments/quarter_1/recom_sys/assignment2/endomondoHR_proper.json') as f:\n",
    "    for i,l in tqdm(enumerate(f)):\n",
    "        if i == 5000:\n",
    "            break\n",
    "        data.append(eval(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7c3aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data\n",
      "167783\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(5) \n",
    "def process(line):\n",
    "    return eval(line)\n",
    "\n",
    "with open('/Users/vedant_j/Desktop/ucsd_assignments/quarter_1/recom_sys/assignment2/endomondoHR_proper.json', 'r') as f:\n",
    "    original_data = pool.map(process, f)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(\"Read data\")\n",
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40fa4b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('longitude',\n",
       "  'altitude',\n",
       "  'latitude',\n",
       "  'sport',\n",
       "  'id',\n",
       "  'heart_rate',\n",
       "  'gender',\n",
       "  'timestamp',\n",
       "  'url',\n",
       "  'userId',\n",
       "  'speed'),\n",
       " ('longitude',\n",
       "  'altitude',\n",
       "  'latitude',\n",
       "  'sport',\n",
       "  'id',\n",
       "  'heart_rate',\n",
       "  'gender',\n",
       "  'url',\n",
       "  'userId',\n",
       "  'timestamp')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_key = set()\n",
    "for k in original_data:\n",
    "    unique_key.add(tuple(k.keys()))\n",
    "unique_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0abbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = set()\n",
    "for d in data:\n",
    "#     if d['sport'] == 'bike':\n",
    "#         print(d['speed'])\n",
    "    sports.add(d['sport'])\n",
    "\n",
    "print(sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840953d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(yoga_heart_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoga_heart_rate = [i[\"heart_rate\"] for i in data if i[\"sport\"] == \"yoga\"]\n",
    "for i in range(len(yoga_heart_rate)):\n",
    "    plt.plot(yoga_heart_rate[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ff32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoga_heart_rate = [i[\"heart_rate\"] for i in data if i[\"sport\"] == \"treadmill running\"]\n",
    "for i in range(len(yoga_heart_rate)):\n",
    "    plt.plot(yoga_heart_rate[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMeta = []\n",
    "count = 0\n",
    "with open('/Users/vedant_j/Desktop/ucsd_assignments/quarter_1/recom_sys/assignment2/endomondoHR_proper.json') as f:\n",
    "    for i,l in tqdm(enumerate(f)):\n",
    "        dataMeta.append(eval(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total dataset : \",len(dataMeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76caaaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    if 'gz' in path:\n",
    "        f = gzip.open(path, 'rb')\n",
    "        for l in f.readlines():\n",
    "            yield(eval(l.decode('ascii')))\n",
    "    else:\n",
    "        f = open(path, 'rb')\n",
    "        for l in f.readlines():\n",
    "            yield(eval(l))\n",
    "\n",
    "def process(line):\n",
    "    return eval(line)\n",
    "\n",
    "class dataInterpreter(object):\n",
    "    def __init__(self, \n",
    "                 inputAtts, \n",
    "                 targetAtts=['derived_speed'], \n",
    "                 includeUser=True, \n",
    "                 includeSport=False, \n",
    "                 includeGender=False, \n",
    "                 includeTemporal=False, \n",
    "                 fn=\"endomondoHR_proper.json\", \n",
    "                 scaleVals=True, \n",
    "                 trimmed_workout_len=450, \n",
    "                 scaleTargets=\"scaleVals\", \n",
    "                 trainValidTestSplit=[.8,.1,.1], \n",
    "                 zMultiple=5, \n",
    "                 trainValidTestFN=None):\n",
    "        \n",
    "        \n",
    "        self.filename = fn\n",
    "        self.data_path = \"./\"\n",
    "        self.metaDataFn = fn.split(\".\")[0] + \"_metaData.pkl\"\n",
    "\n",
    "        self.scaleVals = scaleVals\n",
    "        self.trimmed_workout_len = trimmed_workout_len\n",
    "        if scaleTargets == \"scaleVals\":\n",
    "            scaleTargets = scaleVals\n",
    "        self.scale_targets = scaleTargets # set to false when scale only inputs\n",
    "        self.smooth_window = 1 # window size = 1 means no smoothing\n",
    "        self.perform_target_smoothing = True\n",
    "\n",
    "        self.isNominal = ['gender', 'sport']\n",
    "        self.isDerived = ['time_elapsed', 'distance', 'derived_speed', 'since_begin', 'since_last']\n",
    "        self.isSequence = ['altitude', 'heart_rate', 'latitude', 'longitude'] + self.isDerived\n",
    "\n",
    "        self.inputAtts = inputAtts\n",
    "        self.includeUser = includeUser\n",
    "        self.includeSport = includeSport\n",
    "        self.includeGender = includeGender\n",
    "        self.includeTemporal = includeTemporal\n",
    "\n",
    "        self.targetAtts = [\"tar_\" + tAtt for tAtt in targetAtts]\n",
    "\n",
    "        print(\"input attributes: \", self.inputAtts)\n",
    "        print(\"target attributes: \", self.targetAtts)\n",
    "\n",
    "        self.trainValidTestSplit = trainValidTestSplit\n",
    "        self.trainValidTestFN = trainValidTestFN\n",
    "        self.zMultiple = zMultiple\n",
    "\n",
    "    def preprocess_data(self):\n",
    " \n",
    "        self.original_data_path = self.data_path + \"/\" + self.filename \n",
    "        self.processed_path = self.data_path + \"/processed_\" + self.filename.split(\".\")[0] + \".npy\"\n",
    "\n",
    "        # load index for train/valid/test\n",
    "        #self.loadTrainValidTest()\n",
    "\n",
    "        if os.path.exists(self.processed_path):\n",
    "            # preprocessed data already exist\n",
    "            print(\"{} exists\".format(self.processed_path))\n",
    "            self.original_data = np.load(self.processed_path)[0]\n",
    "            self.map_workout_id()\n",
    "        else:\n",
    "            # not preprocessed yet, load raw data and preprocess\n",
    "            print(\"load original data\")\n",
    "            pool = Pool(5) \n",
    "            with open(self.original_data_path, 'r') as f:\n",
    "                self.original_data =pool.map(process, f)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            self.map_workout_id()\n",
    "            # derive data\n",
    "            self.derive_data()\n",
    "            # build meta\n",
    "            self.buildMetaData()\n",
    "            # scale data\n",
    "            self.scale_data()\n",
    "        \n",
    "        self.load_meta()\n",
    "        self.input_dim = len(self.inputAtts)\n",
    "        self.output_dim = len(self.targetAtts) # each continuous target has dimension 1, so total length = total dimension\n",
    "      \n",
    "    def map_workout_id(self):\n",
    "        # convert workout id to original data id\n",
    "        self.idxMap = defaultdict(int)\n",
    "        for idx, d in enumerate(self.original_data):  \n",
    "            self.idxMap[d['id']] = idx\n",
    "\n",
    "        self.trainingSet = [self.idxMap[wid] for wid in self.trainingSet]\n",
    "        self.validationSet = [self.idxMap[wid] for wid in self.validationSet]\n",
    "        self.testSet = [self.idxMap[wid] for wid in self.testSet]\n",
    "        \n",
    "        # update workout id to index in original_data\n",
    "        contextMap2 = {} \n",
    "        for wid in self.contextMap:\n",
    "            context = self.contextMap[wid]\n",
    "            contextMap2[self.idxMap[wid]] = (context[0], context[1], [self.idxMap[wid] for wid in context[2]])\n",
    "        self.contextMap = contextMap2 \n",
    "    \n",
    "    \n",
    "    def load_meta(self): \n",
    "        self.buildMetaData() \n",
    "\n",
    "    def randomizeDataOrder(self, dataIndices):\n",
    "        return np.random.permutation(dataIndices)\n",
    "\n",
    "    \n",
    "    def generateByIdx(self, index):\n",
    "        targetAtts = self.targetAtts\n",
    "        inputAtts = self.inputAtts\n",
    "\n",
    "        inputDataDim = self.input_dim\n",
    "        targetDataDim = self.output_dim\n",
    "        \n",
    "        current_input = self.original_data[index] \n",
    "        workoutid = current_input['id']\n",
    "\n",
    "        inputs = np.zeros([inputDataDim, self.trimmed_workout_len])\n",
    "        outputs = np.zeros([targetDataDim, self.trimmed_workout_len])\n",
    "        for idx, att in enumerate(inputAtts):\n",
    "            if att == 'time_elapsed':\n",
    "                inputs[idx, :] = np.ones([1, self.trimmed_workout_len]) * current_input[att][self.trimmed_workout_len-1] # given the total workout length\n",
    "            else:\n",
    "                inputs[idx, :] = current_input[att][:self.trimmed_workout_len]\n",
    "        for att in targetAtts:\n",
    "            outputs[0, :] = current_input[att][:self.trimmed_workout_len]\n",
    "        inputs = np.transpose(inputs)\n",
    "        outputs = np.transpose(outputs)\n",
    "\n",
    "        if self.includeUser:\n",
    "            user_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['userId'][current_input['userId']]\n",
    "        if self.includeSport:\n",
    "            sport_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['sport'][current_input['sport']]\n",
    "        if self.includeGender:\n",
    "            gender_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['gender'][current_input['gender']]\n",
    "\n",
    "        # build context input    \n",
    "        if self.includeTemporal:\n",
    "            context_idx = self.contextMap[idx][2][-1] # index of previous workouts\n",
    "            context_input = self.original_data[context_idx]\n",
    "\n",
    "            context_since_last = np.ones([1, self.trimmed_workout_len]) * self.contextMap[idx][0]\n",
    "            # consider what context?\n",
    "            context_inputs = np.zeros([inputDataDim, self.trimmed_workout_len])\n",
    "            context_outputs = np.zeros([targetDataDim, self.trimmed_workout_len])\n",
    "            for idx, att in enumerate(inputAtts):\n",
    "                if att == 'time_elapsed':\n",
    "                    context_inputs[idx, :] = np.ones([1, self.trimmed_workout_len]) * context_input[att][self.trimmed_workout_len-1]\n",
    "                else:\n",
    "                    context_inputs[idx, :] = context_input[att][:self.trimmed_workout_len]\n",
    "            for att in targetAtts:\n",
    "                context_outputs[0, :] = context_input[att][:self.trimmed_workout_len]\n",
    "            context_input_1 = np.transpose(np.concatenate([context_inputs, context_since_last], axis=0))\n",
    "            context_input_2 = np.transpose(context_outputs)\n",
    "\n",
    "        inputs_dict = {'input':inputs}\n",
    "        if self.includeUser:       \n",
    "            inputs_dict['user_input'] = user_inputs\n",
    "        if self.includeSport:       \n",
    "            inputs_dict['sport_input'] = sport_inputs\n",
    "        if self.includeGender:\n",
    "            inputs_dict['gender_input'] = gender_inputs\n",
    "        if self.includeTemporal:\n",
    "            inputs_dict['context_input_1'] = context_input_1\n",
    "            inputs_dict['context_input_2'] = context_input_2\n",
    "\n",
    "        return (inputs_dict, outputs, workoutid)\n",
    "    \n",
    "    # yield input and target data\n",
    "    def dataIteratorSupervised(self, trainValidTest):\n",
    "        targetAtts = self.targetAtts\n",
    "        inputAtts = self.inputAtts\n",
    "\n",
    "        inputDataDim = self.input_dim\n",
    "        targetDataDim = self.output_dim\n",
    "\n",
    "        # run on train, valid or test?\n",
    "        if trainValidTest == 'train':\n",
    "            indices = self.trainingSet\n",
    "        elif trainValidTest == 'valid':\n",
    "            indices = self.validationSet\n",
    "        elif trainValidTest == 'test':\n",
    "            indices = self.testSet\n",
    "        else:\n",
    "            raise (Exception(\"invalid dataset type: must be 'train', 'valid', or 'test'\"))\n",
    "\n",
    "        # loop each data point\n",
    "        for idx in indices:\n",
    "            current_input = self.original_data[idx] \n",
    "            workoutid = current_input['id']\n",
    " \n",
    "            inputs = np.zeros([inputDataDim, self.trimmed_workout_len])\n",
    "            outputs = np.zeros([targetDataDim, self.trimmed_workout_len])\n",
    "            for i, att in enumerate(inputAtts):\n",
    "                if att == 'time_elapsed':\n",
    "                    inputs[i, :] = np.ones([1, self.trimmed_workout_len]) * current_input[att][self.trimmed_workout_len-1] # given the total workout length\n",
    "                else:\n",
    "                    inputs[i, :] = current_input[att][:self.trimmed_workout_len]\n",
    "            for att in targetAtts:\n",
    "                outputs[0, :] = current_input[att][:self.trimmed_workout_len]\n",
    "            inputs = np.transpose(inputs)\n",
    "            outputs = np.transpose(outputs)\n",
    "\n",
    "            if self.includeUser:\n",
    "                user_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['userId'][current_input['userId']]\n",
    "            if self.includeSport:\n",
    "                sport_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['sport'][current_input['sport']]\n",
    "            if self.includeGender:\n",
    "                gender_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['gender'][current_input['gender']]\n",
    "   \n",
    "            # build context input    \n",
    "            if self.includeTemporal:\n",
    "                context_idx = self.contextMap[idx][2][-1] # index of previous workouts\n",
    "                context_input = self.original_data[context_idx]\n",
    "\n",
    "                context_since_last = np.ones([1, self.trimmed_workout_len]) * self.contextMap[idx][0]\n",
    "                # consider what context?\n",
    "                context_inputs = np.zeros([inputDataDim, self.trimmed_workout_len])\n",
    "                context_outputs = np.zeros([targetDataDim, self.trimmed_workout_len])\n",
    "                for i, att in enumerate(inputAtts):\n",
    "                    if att == 'time_elapsed':\n",
    "                        context_inputs[i, :] = np.ones([1, self.trimmed_workout_len]) * context_input[att][self.trimmed_workout_len-1]\n",
    "                    else:\n",
    "                        context_inputs[i, :] = context_input[att][:self.trimmed_workout_len]\n",
    "                for att in targetAtts:\n",
    "                    context_outputs[0, :] = context_input[att][:self.trimmed_workout_len]\n",
    "                context_input_1 = np.transpose(np.concatenate([context_inputs, context_since_last], axis=0))\n",
    "                context_input_2 = np.transpose(context_outputs)\n",
    "            \n",
    "            inputs_dict = {'input':inputs}\n",
    "            if self.includeUser:       \n",
    "                inputs_dict['user_input'] = user_inputs\n",
    "            if self.includeSport:       \n",
    "                inputs_dict['sport_input'] = sport_inputs\n",
    "            if self.includeGender:\n",
    "                inputs_dict['gender_input'] = gender_inputs\n",
    "            if self.includeTemporal:\n",
    "                inputs_dict['context_input_1'] = context_input_1\n",
    "                inputs_dict['context_input_2'] = context_input_2\n",
    "                \n",
    "            yield (inputs_dict, outputs, workoutid)\n",
    "\n",
    "\n",
    "    # feed into Keras' fit_generator (automatically resets)\n",
    "    def generator_for_autotrain(self, batch_size, num_steps, trainValidTest):\n",
    "        print(\"batch size = {}, num steps = {}\".format(batch_size, num_steps))\n",
    "        print(\"start new generator epoch: \" + trainValidTest)\n",
    "\n",
    "        # get the batch generator based on mode: train/valid/test\n",
    "        if trainValidTest==\"train\":\n",
    "            data_len = len(self.trainingSet)\n",
    "        elif trainValidTest==\"valid\":\n",
    "            data_len = len(self.validationSet)\n",
    "        elif trainValidTest==\"test\":\n",
    "            data_len = len(self.testSet)\n",
    "        else:\n",
    "            raise(ValueError(\"trainValidTest is not a valid value\"))\n",
    "        batchGen = self.dataIteratorSupervised(trainValidTest)\n",
    "        epoch_size = int(data_len / batch_size)\n",
    "        inputDataDim = self.input_dim\n",
    "        targetDataDim = self.output_dim\n",
    "\n",
    "        if epoch_size == 0:\n",
    "            raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "            \n",
    "        for i in range(epoch_size):\n",
    "            inputs = np.zeros([batch_size, num_steps, inputDataDim])\n",
    "            outputs = np.zeros([batch_size, num_steps, targetDataDim])\n",
    "            workoutids = np.zeros([batch_size])\n",
    "\n",
    "            if self.includeUser:\n",
    "                user_inputs = np.zeros([batch_size, num_steps, 1])\n",
    "            if self.includeSport:\n",
    "                sport_inputs = np.zeros([batch_size, num_steps, 1])\n",
    "            if self.includeGender:\n",
    "                gender_inputs = np.zeros([batch_size, num_steps, 1])\n",
    "            if self.includeTemporal:\n",
    "                context_input_1 = np.zeros([batch_size, num_steps, inputDataDim + 1])\n",
    "                context_input_2 = np.zeros([batch_size, num_steps, targetDataDim])\n",
    "\n",
    "            # inputs_dict = {'input':inputs}\n",
    "            inputs_dict = {'input':inputs, 'workoutid':workoutids}\n",
    "            for j in range(batch_size):\n",
    "                current = next(batchGen)\n",
    "                inputs[j,:,:] = current[0]['input']\n",
    "                outputs[j,:,:] = current[1]\n",
    "                workoutids[j] = current[2]\n",
    "\n",
    "                if self.includeUser:\n",
    "                    user_inputs[j,:,:] = current[0]['user_input']\n",
    "                    inputs_dict['user_input'] = user_inputs\n",
    "                if self.includeSport:\n",
    "                    sport_inputs[j,:,:] = current[0]['sport_input']\n",
    "                    inputs_dict['sport_input'] = sport_inputs\n",
    "                if self.includeGender:\n",
    "                    gender_inputs[j,:,:] = current[0]['gender_input']\n",
    "                    inputs_dict['gender_input'] = gender_inputs\n",
    "                if self.includeTemporal:\n",
    "                    context_input_1[j,:,:] = current[0]['context_input_1']\n",
    "                    context_input_2[j,:,:] = current[0]['context_input_2']\n",
    "                    inputs_dict['context_input_1'] = context_input_1\n",
    "                    inputs_dict['context_input_2'] = context_input_2\n",
    "            # yield one batch\n",
    "            yield (inputs_dict, outputs)\n",
    "\n",
    "    def loadTrainValidTest(self):\n",
    "        with open(self.trainValidTestFN, \"rb\") as f:\n",
    "            self.trainingSet, self.validationSet, self.testSet, self.contextMap = pickle.load(f)\n",
    "            print(\"train/valid/test set size = {}/{}/{}\".format(len(self.trainingSet), len(self.validationSet), len(self.testSet)))\n",
    "            print(\"dataset split loaded\")       \n",
    "\n",
    "    # derive 'time_elapsed', 'distance', 'new_workout', 'derived_speed'\n",
    "    def deriveData(self, att, currentDataPoint, idx):\n",
    "        if att == 'time_elapsed':\n",
    "            # Derive the time elapsed from the start\n",
    "            timestamps = currentDataPoint['timestamp']\n",
    "            initialTime = timestamps[0]\n",
    "            return [x - initialTime for x in timestamps]\n",
    "        elif att == 'distance':\n",
    "            # Derive the distance\n",
    "            lats = currentDataPoint['latitude']\n",
    "            longs = currentDataPoint['longitude']\n",
    "            indices = range(1, len(lats)) \n",
    "            distances = [0]\n",
    "            # Gets distance traveled since last time point in kilometers\n",
    "            distances.extend([haversine([lats[i-1],longs[i-1]], [lats[i],longs[i]]) for i in indices]) \n",
    "            return distances\n",
    "        # derive the new_workout list\n",
    "        elif att == 'new_workout': \n",
    "            workoutLength = self.trimmed_workout_len\n",
    "            newWorkout = np.zeros(workoutLength)\n",
    "            # Add the signal at start\n",
    "            newWorkout[0] = 1 \n",
    "            return newWorkout\n",
    "        elif att == 'derived_speed':\n",
    "            distances = self.deriveData('distance', currentDataPoint, idx)\n",
    "            timestamps = currentDataPoint['timestamp']\n",
    "            indices = range(1, len(timestamps))\n",
    "            times = [0]\n",
    "            times.extend([timestamps[i] - timestamps[i-1] for i in indices])\n",
    "            derivedSpeeds = [0]\n",
    "            for i in indices:\n",
    "                try:\n",
    "                    curr_speed = 3600 * distances[i] / times[i]\n",
    "                    derivedSpeeds.append(curr_speed)\n",
    "                except:\n",
    "                    derivedSpeeds.append(derivedSpeeds[i-1])\n",
    "            return derivedSpeeds\n",
    "        elif att == 'since_last':\n",
    "            if idx in self.contextMap:\n",
    "                total_time = self.contextMap[idx][0]\n",
    "            else:\n",
    "                total_time = 0\n",
    "            return np.ones(self.trimmed_workout_len) * total_time\n",
    "        elif att == 'since_begin':\n",
    "            if idx in self.contextMap:\n",
    "                total_time = self.contextMap[idx][1]\n",
    "            else:\n",
    "                total_time = 0\n",
    "            return np.ones(self.trimmed_workout_len) * total_time\n",
    "        else:\n",
    "            raise(Exception(\"No such derived data attribute\"))\n",
    "\n",
    "        \n",
    "    # computing z-scores and multiplying them based on a scaling paramater\n",
    "    # produces zero-centered data, which is important for the drop-in procedure\n",
    "    def scaleData(self, data, att, zMultiple=2):\n",
    "        mean, std = self.variableMeans[att], self.variableStds[att]\n",
    "        diff = [d - mean for d in data]\n",
    "        zScore = [d / std for d in diff] \n",
    "        return [x * zMultiple for x in zScore]\n",
    "\n",
    "    # perform fixed-window median smoothing on a sequence\n",
    "    def median_smoothing(self, seq, context_size):\n",
    "        # seq is a list\n",
    "        if context_size == 1: # if the window is 1, no smoothing should be applied\n",
    "            return seq\n",
    "        seq_len = len(seq)\n",
    "        if context_size % 2 == f0:\n",
    "            raise(exception(\"Context size must be odd for median smoothing\"))\n",
    "\n",
    "        smoothed_seq = []\n",
    "        # loop through sequence and smooth each position\n",
    "        for i in range(seq_len): \n",
    "            cont_diff = (context_size - 1) / 2\n",
    "            context_min = int(max(0, i-cont_diff))\n",
    "            context_max = int(min(seq_len, i+cont_diff))\n",
    "            median_val = np.median(seq[context_min:context_max])\n",
    "            smoothed_seq.append(median_val)\n",
    "\n",
    "        return smoothed_seq\n",
    "    \n",
    "    def buildEncoder(self, classLabels):\n",
    "        # Constructs a dictionary that maps each class label to a list \n",
    "        # where one entry in the list is 1 and the remainder are 0\n",
    "        encodingLength = len(classLabels)\n",
    "        encoder = {}\n",
    "        mapper = {}\n",
    "        for i, label in enumerate(classLabels):\n",
    "            encoding = [0] * encodingLength\n",
    "            encoding[i] = 1\n",
    "            encoder[label] = encoding\n",
    "            mapper[label] = i\n",
    "        return encoder, mapper\n",
    "    \n",
    "    \n",
    "    def writeSummaryFile(self):\n",
    "        metaDataForWriting=metaDataEndomondo(self.numDataPoints, self.encodingLengths, self.oneHotEncoders,  \n",
    "                                             self.oneHotMap, self.isSequence, self.isNominal, self.isDerived, \n",
    "                                             self.variableMeans, self.variableStds)\n",
    "        with open(self.metaDataFn, \"wb\") as f:\n",
    "            pickle.dump(metaDataForWriting, f)\n",
    "        print(\"Summary file written\")\n",
    "        \n",
    "    def loadSummaryFile(self):\n",
    "        try:\n",
    "            print(\"Loading metadata\")\n",
    "            with open(self.metaDataFn, \"rb\") as f:\n",
    "                metaData = pickle.load(f)\n",
    "        except:\n",
    "            raise(IOError(\"Metadata file: \" + self.metaDataFn + \" not in valid pickle format\"))\n",
    "        self.numDataPoints = metaData.numDataPoints\n",
    "        self.encodingLengths = metaData.encodingLengths\n",
    "        self.oneHotEncoders = metaData.oneHotEncoders\n",
    "        self.oneHotMap = metaData.oneHotMap\n",
    "        self.isSequence = metaData.isSequence \n",
    "        self.isNominal = metaData.isNominal\n",
    "        self.variableMeans = metaData.variableMeans\n",
    "        self.variableStds = metaData.variableStds\n",
    "        print(\"Metadata loaded\")\n",
    "\n",
    "        \n",
    "    def derive_data(self):\n",
    "        print(\"derive data\")\n",
    "        # derive based on original data\n",
    "        for idx, d in enumerate(self.original_data):\n",
    "            for att in self.isDerived:\n",
    "                self.original_data[idx][att] = self.deriveData(att, d, idx) # add derived attribute\n",
    "            \n",
    "        \n",
    "    # Generate meta information about data\n",
    "    def buildMetaData(self):\n",
    "        if os.path.isfile(self.metaDataFn):\n",
    "            self.loadSummaryFile()\n",
    "        else:\n",
    "            print(\"Building data schema\")\n",
    "            # other than categoriacl, all are continuous\n",
    "            # categorical to one-hot: gender, sport\n",
    "            # categorical to embedding: userId  \n",
    "            \n",
    "            # continuous attributes\n",
    "            print(\"is sequence: {}\".format(self.isSequence))  \n",
    "            # sum of variables? \n",
    "            variableSums = defaultdict(float)\n",
    "            \n",
    "            # number of categories for each categorical variable\n",
    "            classLabels = defaultdict(set)\n",
    "        \n",
    "            # consider all data to first get the max, min, etc...      \n",
    "            for currData in self.original_data:\n",
    "                # update number of users\n",
    "                att = 'userId'\n",
    "                user = currData[att]\n",
    "                classLabels[att].add(user)\n",
    "                \n",
    "                # update categorical attribute\n",
    "                for att in self.isNominal:\n",
    "                    val  = currData[att]\n",
    "                    classLabels[att].add(val)\n",
    "                    \n",
    "                # update continuous attribute\n",
    "                for att in self.isSequence: \n",
    "                    variableSums[att] += sum(currData[att])\n",
    "\n",
    "            oneHotEncoders = {}\n",
    "            oneHotMap = {}\n",
    "            encodingLengths = {}\n",
    "            for att in self.isNominal:\n",
    "                oneHotEncoders[att], oneHotMap[att] = self.buildEncoder(classLabels[att]) \n",
    "                encodingLengths[att] = len(classLabels[att])\n",
    "            \n",
    "            att = 'userId'\n",
    "            oneHotEncoders[att], oneHotMap[att] = self.buildEncoder(classLabels[att]) \n",
    "            encodingLengths[att] = 1\n",
    "            \n",
    "            for att in self.isSequence:\n",
    "                encodingLengths[att] = 1\n",
    "            \n",
    "            # summary information\n",
    "            self.numDataPoints=len(self.original_data)\n",
    "            \n",
    "            # normalize continuous: altitude, heart_rate, latitude, longitude, speed and all derives            \n",
    "            self.computeMeanStd(variableSums, self.numDataPoints, self.isSequence)\n",
    "    \n",
    "            self.oneHotEncoders=oneHotEncoders\n",
    "            self.oneHotMap = oneHotMap\n",
    "            self.encodingLengths = encodingLengths\n",
    "            #Save that summary file so that it can be used next time\n",
    "            self.writeSummaryFile()\n",
    "\n",
    " \n",
    "    def computeMeanStd(self, varSums, numDataPoints, attributes):\n",
    "        print(\"Computing variable means and standard deviations\")\n",
    "        \n",
    "        # assume each data point has 500 time step?! is it correct?\n",
    "        numSequencePoints = numDataPoints * 500 \n",
    "        \n",
    "        variableMeans = {}\n",
    "        for att in varSums:\n",
    "            variableMeans[att] = varSums[att] / numSequencePoints\n",
    "        \n",
    "        varResidualSums = defaultdict(float)\n",
    "        \n",
    "        for numDataPoints, currData in enumerate(self.original_data):\n",
    "            # loop each continuous attribute\n",
    "            for att in attributes:\n",
    "                dataPointArray = np.array(currData[att])\n",
    "                # add to the variable running sum of squared residuals\n",
    "                diff = np.subtract(dataPointArray, variableMeans[att])\n",
    "                sq = np.square(diff)\n",
    "                varResidualSums[att] += np.sum(sq)\n",
    "\n",
    "        variableStds = {}\n",
    "        for att in varResidualSums:\n",
    "            variableStds[att] = np.sqrt(varResidualSums[att] / numSequencePoints)\n",
    "            \n",
    "        self.variableMeans = variableMeans\n",
    "        self.variableStds = variableStds\n",
    "        \n",
    "        \n",
    "    # scale continuous data\n",
    "    def scale_data(self, scaling=True): \n",
    "        print(\"scale data\")\n",
    "        targetAtts = ['heart_rate', 'derived_speed']\n",
    "\n",
    "        for idx, currentDataPoint in enumerate(self.original_data):\n",
    "            # target attribute, add to dict \n",
    "            for tAtt in targetAtts:         \n",
    "                if self.perform_target_smoothing:\n",
    "                    tar_data = self.median_smoothing(currentDataPoint[tAtt], self.smooth_window)\n",
    "                else:\n",
    "                    tar_data = currentDataPoint[tAtt]\n",
    "                if self.scale_targets:\n",
    "                    tar_data = self.scaleData(tar_data, tAtt, self.zMultiple) \n",
    "                self.original_data[idx][\"tar_\" + tAtt] = tar_data\n",
    "                    \n",
    "            # continuous input attribute, update dict\n",
    "            for att in self.isSequence: \n",
    "                if scaling:\n",
    "                    in_data = currentDataPoint[att]\n",
    "                    self.original_data[idx][att] = self.scaleData(in_data, att, self.zMultiple) \n",
    "        for d in self.original_data:\n",
    "            key = 'url'\n",
    "            del d[key]\n",
    "            key = 'speed'\n",
    "            if key in d:\n",
    "                del d[key]\n",
    "        \n",
    "        # write to disk\n",
    "        np.save([self.original_data], self.processed_path)\n",
    "\n",
    "\n",
    "class metaDataEndomondo(object):\n",
    "    def __init__(self, numDataPoints, encodingLengths, oneHotEncoders, oneHotMap, isSequence, isNominal, isDerived,\n",
    "                 variableMeans, variableStds):\n",
    "        self.numDataPoints = numDataPoints\n",
    "        self.encodingLengths = encodingLengths\n",
    "        self.oneHotEncoders = oneHotEncoders\n",
    "        self.oneHotMap = oneHotMap\n",
    "        self.isSequence = isSequence\n",
    "        self.isNominal = isNominal\n",
    "        self.isDerived = isDerived\n",
    "        self.variableMeans = variableMeans\n",
    "        self.variableStds = variableStds\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Base data path....\n",
    "    data_path = \"endomondoHR_proper.json\"\n",
    "    attrFeatures = ['userId', 'sport', 'gender']\n",
    "    trainValidTestSplit = [0.8, 0.1, 0.1]\n",
    "    targetAtts = [\"derived_speed\"]\n",
    "    inputAtts = [\"distance\", \"altitude\", \"time_elapsed\"]\n",
    "    \n",
    "    \n",
    "    endo_reader = dataInterpreter(inputAtts)\n",
    "    \n",
    "    endo_reader.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bb69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88afc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
