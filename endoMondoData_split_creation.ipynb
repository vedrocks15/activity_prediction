{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qz6b7F-Wh4z"
   },
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W4Jrb4yJWfTF"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from haversine import haversine\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "\n",
    "#import multiprocessing\n",
    "#from multiprocessing import Pool\n",
    "\n",
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4dM8-HFXgd4"
   },
   "source": [
    "# Creating Dataset Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n8Jo8vlSXsJk"
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "# Derived features\n",
    "def calc_mse(avg, hrs):\n",
    "    dif = (np.array(hrs) - avg) ** 2\n",
    "    return np.mean(dif)\n",
    "\n",
    "# converting timestamps to proper format for model consumption\n",
    "def convert2datetime(unix_timestamp):\n",
    "    utc_time = time.gmtime(unix_timestamp)\n",
    "    l = time.localtime(unix_timestamp)\n",
    "    dt = datetime.datetime(*l[:6])\n",
    "    return dt\n",
    "\n",
    "# Function to read data\n",
    "def process(line):\n",
    "    return eval(line)\n",
    "\n",
    "# Z-normalize feature values\n",
    "def normalize(inp, zMultiple=5):\n",
    "    mean, std = inp.mean(), inp.std()\n",
    "    diff = inp - mean\n",
    "    zScore = diff / std\n",
    "    return zScore * zMultiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZybzVa1uXh8g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded data points : 167783\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset in the multi-processing fashion.......\n",
    "path = Path(\"./\")\n",
    "in_path = str(path / \"endomondoHR_proper.json\")\n",
    "\n",
    "# Multiprocessing data read....\n",
    "pool = mp.Pool(2)\n",
    "with open(in_path, 'r') as f:\n",
    "    data = pool.map(process, f)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(\"Number of loaded data points :\",len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNTpRH6qYSc9"
   },
   "source": [
    "# More dataset filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lk6-ko8zYEDs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique users : 1059\n"
     ]
    }
   ],
   "source": [
    "data2 = data\n",
    "\n",
    "# User to workout id mapper.... {user_id_1 : [w_4,w_5,w_8.......]}\n",
    "user2workout = defaultdict(list)\n",
    "idxMap = {}\n",
    "for idx in range(len(data2)):\n",
    "    d = data2[idx]\n",
    "    wid = d['id']      # workout id\n",
    "    uid = d['userId']  # user    id\n",
    "    user2workout[uid].append(wid)\n",
    "    \n",
    "    # all work out ids are different...\n",
    "    idxMap[wid] = idx\n",
    "\n",
    "print(\"Total number of unique users :\",len(user2workout)) # how many user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number match as mentioned in the [FitRec](https://sites.google.com/eng.ucsd.edu/fitrec-project/home) site**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bp1b-m57YnQH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with alteast 10 workout sessions : 956\n",
      "Total number of workouts in the 10 session threshold : 167373\n"
     ]
    }
   ],
   "source": [
    "# sorting user's workout based on time for a given workout\n",
    "for u in user2workout:\n",
    "    # Workout list...\n",
    "    workout = user2workout[u]\n",
    "    \n",
    "    # Converting timestamp to proper format \n",
    "    dts = [(convert2datetime(data2[idxMap[wid]]['timestamp'][0]), wid) for wid in workout]\n",
    "    dts = sorted(dts, key=lambda x:x[0])\n",
    "    \n",
    "    # ascending\n",
    "    new_workout = [x[1] for x in dts] \n",
    "    \n",
    "    # updating the sorted list\n",
    "    user2workout[u] = new_workout\n",
    "\n",
    "# keep 10 core (keeping only the users that have done atleast 10 workouts..)\n",
    "user2workout_core = defaultdict(list)\n",
    "for u in user2workout:\n",
    "    workout = user2workout[u]\n",
    "    if len(workout) >= 10:\n",
    "        user2workout_core[u] = workout\n",
    "        \n",
    "print(\"Number of users with alteast 10 workout sessions :\",len(user2workout_core))\n",
    "\n",
    "# total workouts in 10 core subset\n",
    "count = 0\n",
    "for u in user2workout_core:\n",
    "    count += len(user2workout_core[u])\n",
    "print(\"Total number of workouts in the 10 session threshold :\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsEZyTfOZJMy"
   },
   "source": [
    "# Mean workout times\n",
    "\n",
    "> Global Mean\n",
    "\n",
    "> User Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RF4Vcg7tY6ZL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print global mean workout time 1.49 hrs\n"
     ]
    }
   ],
   "source": [
    "# build time lines (mean times & total time of workouts)\n",
    "times = defaultdict(float)\n",
    "user_times = defaultdict(float)\n",
    "\n",
    "for u in user2workout_core:\n",
    "    workout = user2workout_core[u]\n",
    "    tt = []\n",
    "    \n",
    "    for wid in workout:\n",
    "        idx = idxMap[wid]\n",
    "        d = data2[idx]\n",
    "        ts = d['timestamp']\n",
    "\n",
    "        # Keeping track of workout lengths..\n",
    "        times[wid] = (ts[-1] - ts[0]) / 3600\n",
    "        tt.append((ts[-1] - ts[0]) / 3600)\n",
    "    tt = np.array(tt).mean()\n",
    "\n",
    "    # Usermean time\n",
    "    user_times[u] = tt\n",
    "\n",
    "# Global mean time....\n",
    "vals = np.array(list(times.values()))\n",
    "print(\"Print global mean workout time {:.2f} hrs\".format(vals.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVbVkICbcoD4"
   },
   "source": [
    "# Previous workouts to current workout\n",
    "\n",
    "Each users first workout is removed therefore:\n",
    "\n",
    "**1,67,373 = 1,66,417 + 956**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kJjREdPacreF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of context map :  166417\n"
     ]
    }
   ],
   "source": [
    "# contextMap stores all workouts previous to current\n",
    "contextMap= {}\n",
    "\n",
    "for u in user2workout_core:\n",
    "    wids = user2workout_core[u]\n",
    "\n",
    "    # workout id to index.....\n",
    "    indices = [idxMap[wid] for wid in wids]\n",
    "\n",
    "    # build start time\n",
    "    start_times = []\n",
    "    for idx in indices:\n",
    "        start_time = data[idx]['timestamp'][0]\n",
    "        start_times.append(start_time)\n",
    "\n",
    "    # build context (0th workout has no i-1 to compare)\n",
    "    for i in range(1, len(wids)):\n",
    "        wid = wids[i]\n",
    "        # Gap between the previous session\n",
    "        since_last  = (start_times[i] - start_times[i-1]) / (3600*24)\n",
    "        \n",
    "        # Gap between the current & first session \n",
    "        since_begin = (start_times[i] - start_times[0]) / (3600*24)\n",
    "        contextMap[wid] = (since_last, since_begin, wids[:i])\n",
    "\n",
    "print(\"Size of context map : \",len(contextMap))\n",
    "\n",
    "since_last_array = []\n",
    "since_begin_array = []\n",
    "for wid in contextMap:\n",
    "    t = contextMap[wid]\n",
    "    since_last_array.append(t[0])\n",
    "    since_begin_array.append(t[1])\n",
    "\n",
    "since_last_array   = np.array(since_last_array)\n",
    "since_begin_array  = np.array(since_begin_array)\n",
    "\n",
    "# normalize since last and begin\n",
    "since_last_array2  = normalize(since_last_array)\n",
    "since_begin_array2 = normalize(since_begin_array)\n",
    "\n",
    "# put nomalized since last and begin into contextMap\n",
    "i = 0\n",
    "contextMap2 = {}\n",
    "for wid in contextMap:\n",
    "    t = contextMap[wid]\n",
    "    t0 = since_last_array2[i]\n",
    "    t1 = since_begin_array2[i]\n",
    "    i += 1\n",
    "    contextMap2[wid] = (t0, t1, t[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "389-ilyYdlzA"
   },
   "source": [
    "# Final Split of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CEpp63M0dRMh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits : train/valid/test = 132755/16604/17058\n"
     ]
    }
   ],
   "source": [
    "# split whole dataset, leave latest into valid and test\n",
    "train,valid,test = [],[],[]\n",
    "for u in user2workout_core:\n",
    "    indices = user2workout_core[u][1:] # remove the first workout since it has no context\n",
    "    l = len(indices)\n",
    "    # split in ascending order\n",
    "    train.extend(indices[:int(0.8*l)])\n",
    "    valid.extend(indices[int(0.8*l):int(0.9*l)])\n",
    "    test.extend(indices[int(0.9*l):])\n",
    "\n",
    "print(\"Splits : train/valid/test = {}/{}/{}\".format(len(train), len(valid), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving as PICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('endomondoHR_proper_temporal_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump((train,valid,test,contextMap2), f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
