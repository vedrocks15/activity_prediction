{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341fdae0",
   "metadata": {},
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3021a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from haversine import haversine\n",
    "from math import floor\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import gzip\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocess as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3067b4d",
   "metadata": {},
   "source": [
    "# Setting up initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b78ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned endomondo dataset.... (simply cleaned for cases with abnormal readings..)\n",
    "data_path = \"endomondoHR_proper.json\"\n",
    "\n",
    "# Attribute embedding features....\n",
    "attrFeatures = ['userId', 'sport', 'gender']\n",
    "    \n",
    "# Percentage of data splits....\n",
    "trainValidTestSplit = [0.8, 0.1, 0.1]\n",
    "targetAtts = [\"derived_speed\"]\n",
    "    \n",
    "# Time sequnce inputs.... (contextual features...)\n",
    "inputAtts = [\"distance\", \"altitude\", \"time_elapsed\"]\n",
    "\n",
    "# splits file\n",
    "trainValidTestFN = \"./endomondoHR_proper_temporal_dataset.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f0bf0a",
   "metadata": {},
   "source": [
    "# Data reading helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae16813",
   "metadata": {
    "code_folding": [
     1,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# dataset already been preprocessed\n",
    "def parse(path):\n",
    "    if 'gz' in path:\n",
    "        f = gzip.open(path, 'rb')\n",
    "        for l in f.readlines():\n",
    "            yield(eval(l.decode('ascii')))\n",
    "    else:\n",
    "        f = open(path, 'rb')\n",
    "        for l in f.readlines():\n",
    "            yield(eval(l))\n",
    "\n",
    "def process(line):\n",
    "    return eval(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc56058",
   "metadata": {},
   "source": [
    "# Pre-processing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b16d1e",
   "metadata": {
    "code_folding": [
     0,
     3,
     66,
     116,
     142,
     145,
     149,
     211,
     286,
     348,
     356,
     358,
     365,
     378,
     389,
     410,
     420,
     426,
     433,
     440,
     459,
     473,
     481,
     499,
     511,
     533,
     554,
     564,
     581,
     632
    ]
   },
   "outputs": [],
   "source": [
    "class dataInterpreter(object):\n",
    "    \n",
    "    # Basic constructor \n",
    "    def __init__(self, \n",
    "                 inputAtts, \n",
    "                 targetAtts=['derived_speed'], \n",
    "                 includeUser=True, \n",
    "                 includeSport=False, \n",
    "                 includeGender=False, \n",
    "                 includeTemporal=False, \n",
    "                 fn=\"endomondoHR_proper.json\", \n",
    "                 scaleVals=True, \n",
    "                 trimmed_workout_len=450, \n",
    "                 scaleTargets=\"scaleVals\", \n",
    "                 trainValidTestSplit=[.8,.1,.1], \n",
    "                 zMultiple=5, \n",
    "                 trainValidTestFN=None):\n",
    "\n",
    "        # Loading the proper dataset...   \n",
    "        self.filename = fn\n",
    "        self.data_path = \"./\"\n",
    "        self.metaDataFn = fn.split(\".\")[0] + \"_metaData.pkl\"\n",
    "\n",
    "        # z-normalizing the features...\n",
    "        self.scaleVals = scaleVals\n",
    "\n",
    "        # fixing the number of timesteps for the workout \n",
    "        self.trimmed_workout_len = trimmed_workout_len\n",
    "        \n",
    "        # set to false when scale only inputs\n",
    "        if scaleTargets == \"scaleVals\":\n",
    "            scaleTargets = scaleVals\n",
    "        self.scale_targets = scaleTargets \n",
    "        \n",
    "        # window size = 1 means no smoothing (using contextual information from adjacent points...)\n",
    "        self.smooth_window = 1 \n",
    "        self.perform_target_smoothing = True\n",
    "\n",
    "        # Nominal features with fixed set of values....\n",
    "        self.isNominal = ['gender', 'sport']\n",
    "\n",
    "        # Features that are derived....\n",
    "        self.isDerived = ['time_elapsed', 'distance', 'derived_speed', 'since_begin', 'since_last']\n",
    "\n",
    "        # sequence features...\n",
    "        self.isSequence = ['altitude', 'heart_rate', 'latitude', 'longitude'] + self.isDerived\n",
    "        \n",
    "        # setting up all other feature list \n",
    "        self.inputAtts = inputAtts\n",
    "        self.includeUser = includeUser\n",
    "        self.includeSport = includeSport\n",
    "        self.includeGender = includeGender\n",
    "        self.includeTemporal = includeTemporal\n",
    "\n",
    "        # Target variable....\n",
    "        self.targetAtts = [\"tar_\" + tAtt for tAtt in targetAtts]\n",
    "\n",
    "        print(\"input attributes: \", self.inputAtts)\n",
    "        print(\"target attributes: \", self.targetAtts)\n",
    "\n",
    "        # Setting up data split parameters....\n",
    "        self.trainValidTestSplit = trainValidTestSplit\n",
    "        self.trainValidTestFN = trainValidTestFN\n",
    "        self.zMultiple = zMultiple\n",
    "\n",
    "    # Main function to create new features.....\n",
    "    def preprocess_data(self):\n",
    " \n",
    "        self.original_data_path = self.data_path + \"/\" + self.filename \n",
    "        self.processed_path = self.data_path + \"/processed_\" + self.filename.split(\".\")[0] + \".npy\"\n",
    "\n",
    "        # load index for train/valid/test (already created from the splits notebook)\n",
    "        self.loadTrainValidTest()\n",
    "        \n",
    "        # Checking if already a processed version of the dataset exists or not\n",
    "        if os.path.exists(self.processed_path):\n",
    "            print(\"{} exists\".format(self.processed_path))\n",
    "            self.original_data = np.load(self.processed_path)[0]\n",
    "            self.map_workout_id()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # not preprocessed yet, load raw data and preprocess\n",
    "            print(\"load original data\")\n",
    "            pool = mp.Pool(5) \n",
    "            with open(self.original_data_path, 'r') as f:\n",
    "                self.original_data = pool.map(process, f)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "            print(\"Original dataset with {} points loaded\".format(len(self.original_data)))\n",
    "            \n",
    "            # Create a new index map of workout ids\n",
    "            self.map_workout_id()\n",
    "            print(\"Updated mapping workout id..\")\n",
    "            print(\"##############\")\n",
    "            \n",
    "            # Create new derived features\n",
    "            self.derive_data()\n",
    "            print(\"Created new features..\")\n",
    "            print(\"##############\")\n",
    "            \n",
    "            # build meta\n",
    "            self.buildMetaData()\n",
    "            print(\"Meta Data file generated....\")\n",
    "            print(\"##############\")\n",
    "            \n",
    "            # scale data\n",
    "            self.scale_data()\n",
    "            print(\"Data is scaled....\")\n",
    "            print(\"##############\")\n",
    "        \n",
    "        self.load_meta()\n",
    "        self.input_dim = len(self.inputAtts)\n",
    "        self.output_dim = len(self.targetAtts) # each continuous target has dimension 1, so total length = total dimension\n",
    "      \n",
    "    def map_workout_id(self):\n",
    "        \n",
    "        # convert workout id (session ID) to original data row number\n",
    "        self.idxMap = defaultdict(int)\n",
    "        \n",
    "        # Id to numeric index mapping..... (mapping each session to a number)\n",
    "        for idx, d in enumerate(self.original_data):  \n",
    "            self.idxMap[d['id']] = idx\n",
    "\n",
    "        # splitting the dataset based on session IDS\n",
    "        self.trainingSet = [self.idxMap[wid] for wid in self.trainingSet]\n",
    "        self.validationSet = [self.idxMap[wid] for wid in self.validationSet]\n",
    "        self.testSet = [self.idxMap[wid] for wid in self.testSet]\n",
    "        \n",
    "        # update workout id to index in original_data\n",
    "        contextMap2 = {} \n",
    "        \n",
    "        # previous workout information...\n",
    "        for wid in self.contextMap:\n",
    "            context = self.contextMap[wid]\n",
    "            contextMap2[self.idxMap[wid]] = (context[0], context[1], [self.idxMap[wid] for wid in context[2]])\n",
    "        \n",
    "        # updating context map\n",
    "        self.contextMap = contextMap2 \n",
    "    \n",
    "    \n",
    "    def load_meta(self): \n",
    "        self.buildMetaData() \n",
    "\n",
    "    def randomizeDataOrder(self, dataIndices):\n",
    "        return np.random.permutation(dataIndices)\n",
    "\n",
    "    \n",
    "    def generateByIdx(self, index):\n",
    "        targetAtts = self.targetAtts\n",
    "        inputAtts = self.inputAtts\n",
    "\n",
    "        inputDataDim = self.input_dim\n",
    "        targetDataDim = self.output_dim\n",
    "        \n",
    "        current_input = self.original_data[index] \n",
    "        workoutid = current_input['id']\n",
    "\n",
    "        inputs = np.zeros([inputDataDim, self.trimmed_workout_len])\n",
    "        outputs = np.zeros([targetDataDim, self.trimmed_workout_len])\n",
    "        for idx, att in enumerate(inputAtts):\n",
    "            if att == 'time_elapsed':\n",
    "                inputs[idx, :] = np.ones([1, self.trimmed_workout_len]) * current_input[att][self.trimmed_workout_len-1] # given the total workout length\n",
    "            else:\n",
    "                inputs[idx, :] = current_input[att][:self.trimmed_workout_len]\n",
    "        for att in targetAtts:\n",
    "            outputs[0, :] = current_input[att][:self.trimmed_workout_len]\n",
    "        inputs = np.transpose(inputs)\n",
    "        outputs = np.transpose(outputs)\n",
    "\n",
    "        if self.includeUser:\n",
    "            user_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['userId'][current_input['userId']]\n",
    "        if self.includeSport:\n",
    "            sport_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['sport'][current_input['sport']]\n",
    "        if self.includeGender:\n",
    "            gender_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['gender'][current_input['gender']]\n",
    "\n",
    "        # build context input    \n",
    "        if self.includeTemporal:\n",
    "            context_idx = self.contextMap[idx][2][-1] # index of previous workouts\n",
    "            context_input = self.original_data[context_idx]\n",
    "\n",
    "            context_since_last = np.ones([1, self.trimmed_workout_len]) * self.contextMap[idx][0]\n",
    "            # consider what context?\n",
    "            context_inputs = np.zeros([inputDataDim, self.trimmed_workout_len])\n",
    "            context_outputs = np.zeros([targetDataDim, self.trimmed_workout_len])\n",
    "            for idx, att in enumerate(inputAtts):\n",
    "                if att == 'time_elapsed':\n",
    "                    context_inputs[idx, :] = np.ones([1, self.trimmed_workout_len]) * context_input[att][self.trimmed_workout_len-1]\n",
    "                else:\n",
    "                    context_inputs[idx, :] = context_input[att][:self.trimmed_workout_len]\n",
    "            for att in targetAtts:\n",
    "                context_outputs[0, :] = context_input[att][:self.trimmed_workout_len]\n",
    "            context_input_1 = np.transpose(np.concatenate([context_inputs, context_since_last], axis=0))\n",
    "            context_input_2 = np.transpose(context_outputs)\n",
    "\n",
    "        inputs_dict = {'input':inputs}\n",
    "        if self.includeUser:       \n",
    "            inputs_dict['user_input'] = user_inputs\n",
    "        if self.includeSport:       \n",
    "            inputs_dict['sport_input'] = sport_inputs\n",
    "        if self.includeGender:\n",
    "            inputs_dict['gender_input'] = gender_inputs\n",
    "        if self.includeTemporal:\n",
    "            inputs_dict['context_input_1'] = context_input_1\n",
    "            inputs_dict['context_input_2'] = context_input_2\n",
    "\n",
    "        return (inputs_dict, outputs, workoutid)\n",
    "    \n",
    "    # yield input and target data\n",
    "    def dataIteratorSupervised(self, trainValidTest):\n",
    "        targetAtts = self.targetAtts\n",
    "        inputAtts = self.inputAtts\n",
    "\n",
    "        inputDataDim = self.input_dim\n",
    "        targetDataDim = self.output_dim\n",
    "\n",
    "        # run on train, valid or test?\n",
    "        if trainValidTest == 'train':\n",
    "            indices = self.trainingSet\n",
    "        elif trainValidTest == 'valid':\n",
    "            indices = self.validationSet\n",
    "        elif trainValidTest == 'test':\n",
    "            indices = self.testSet\n",
    "        else:\n",
    "            raise (Exception(\"invalid dataset type: must be 'train', 'valid', or 'test'\"))\n",
    "\n",
    "        # loop each data point\n",
    "        for idx in indices:\n",
    "            current_input = self.original_data[idx] \n",
    "            workoutid = current_input['id']\n",
    " \n",
    "            inputs = np.zeros([inputDataDim, self.trimmed_workout_len])\n",
    "            outputs = np.zeros([targetDataDim, self.trimmed_workout_len])\n",
    "            for i, att in enumerate(inputAtts):\n",
    "                if att == 'time_elapsed':\n",
    "                    inputs[i, :] = np.ones([1, self.trimmed_workout_len]) * current_input[att][self.trimmed_workout_len-1] # given the total workout length\n",
    "                else:\n",
    "                    inputs[i, :] = current_input[att][:self.trimmed_workout_len]\n",
    "            for att in targetAtts:\n",
    "                outputs[0, :] = current_input[att][:self.trimmed_workout_len]\n",
    "            inputs = np.transpose(inputs)\n",
    "            outputs = np.transpose(outputs)\n",
    "\n",
    "            if self.includeUser:\n",
    "                user_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['userId'][current_input['userId']]\n",
    "            if self.includeSport:\n",
    "                sport_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['sport'][current_input['sport']]\n",
    "            if self.includeGender:\n",
    "                gender_inputs = np.ones([self.trimmed_workout_len, 1]) * self.oneHotMap['gender'][current_input['gender']]\n",
    "   \n",
    "            # build context input    \n",
    "            if self.includeTemporal:\n",
    "                context_idx = self.contextMap[idx][2][-1] # index of previous workouts\n",
    "                context_input = self.original_data[context_idx]\n",
    "\n",
    "                context_since_last = np.ones([1, self.trimmed_workout_len]) * self.contextMap[idx][0]\n",
    "                # consider what context?\n",
    "                context_inputs = np.zeros([inputDataDim, self.trimmed_workout_len])\n",
    "                context_outputs = np.zeros([targetDataDim, self.trimmed_workout_len])\n",
    "                for i, att in enumerate(inputAtts):\n",
    "                    if att == 'time_elapsed':\n",
    "                        context_inputs[i, :] = np.ones([1, self.trimmed_workout_len]) * context_input[att][self.trimmed_workout_len-1]\n",
    "                    else:\n",
    "                        context_inputs[i, :] = context_input[att][:self.trimmed_workout_len]\n",
    "                for att in targetAtts:\n",
    "                    context_outputs[0, :] = context_input[att][:self.trimmed_workout_len]\n",
    "                context_input_1 = np.transpose(np.concatenate([context_inputs, context_since_last], axis=0))\n",
    "                context_input_2 = np.transpose(context_outputs)\n",
    "            \n",
    "            inputs_dict = {'input':inputs}\n",
    "            if self.includeUser:       \n",
    "                inputs_dict['user_input'] = user_inputs\n",
    "            if self.includeSport:       \n",
    "                inputs_dict['sport_input'] = sport_inputs\n",
    "            if self.includeGender:\n",
    "                inputs_dict['gender_input'] = gender_inputs\n",
    "            if self.includeTemporal:\n",
    "                inputs_dict['context_input_1'] = context_input_1\n",
    "                inputs_dict['context_input_2'] = context_input_2\n",
    "                \n",
    "            yield (inputs_dict, outputs, workoutid)\n",
    "\n",
    "\n",
    "    # feed into Keras' fit_generator (automatically resets)\n",
    "    def generator_for_autotrain(self, batch_size, num_steps, trainValidTest):\n",
    "        print(\"batch size = {}, num steps = {}\".format(batch_size, num_steps))\n",
    "        print(\"start new generator epoch: \" + trainValidTest)\n",
    "\n",
    "        # get the batch generator based on mode: train/valid/test\n",
    "        if trainValidTest==\"train\":\n",
    "            data_len = len(self.trainingSet)\n",
    "        elif trainValidTest==\"valid\":\n",
    "            data_len = len(self.validationSet)\n",
    "        elif trainValidTest==\"test\":\n",
    "            data_len = len(self.testSet)\n",
    "        else:\n",
    "            raise(ValueError(\"trainValidTest is not a valid value\"))\n",
    "        batchGen = self.dataIteratorSupervised(trainValidTest)\n",
    "        epoch_size = int(data_len / batch_size)\n",
    "        inputDataDim = self.input_dim\n",
    "        targetDataDim = self.output_dim\n",
    "\n",
    "        if epoch_size == 0:\n",
    "            raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "            \n",
    "        for i in range(epoch_size):\n",
    "            inputs = np.zeros([batch_size, num_steps, inputDataDim])\n",
    "            outputs = np.zeros([batch_size, num_steps, targetDataDim])\n",
    "            workoutids = np.zeros([batch_size])\n",
    "\n",
    "            if self.includeUser:\n",
    "                user_inputs = np.zeros([batch_size, num_steps, 1])\n",
    "            if self.includeSport:\n",
    "                sport_inputs = np.zeros([batch_size, num_steps, 1])\n",
    "            if self.includeGender:\n",
    "                gender_inputs = np.zeros([batch_size, num_steps, 1])\n",
    "            if self.includeTemporal:\n",
    "                context_input_1 = np.zeros([batch_size, num_steps, inputDataDim + 1])\n",
    "                context_input_2 = np.zeros([batch_size, num_steps, targetDataDim])\n",
    "\n",
    "            # inputs_dict = {'input':inputs}\n",
    "            inputs_dict = {'input':inputs, 'workoutid':workoutids}\n",
    "            for j in range(batch_size):\n",
    "                current = next(batchGen)\n",
    "                inputs[j,:,:] = current[0]['input']\n",
    "                outputs[j,:,:] = current[1]\n",
    "                workoutids[j] = current[2]\n",
    "\n",
    "                if self.includeUser:\n",
    "                    user_inputs[j,:,:] = current[0]['user_input']\n",
    "                    inputs_dict['user_input'] = user_inputs\n",
    "                if self.includeSport:\n",
    "                    sport_inputs[j,:,:] = current[0]['sport_input']\n",
    "                    inputs_dict['sport_input'] = sport_inputs\n",
    "                if self.includeGender:\n",
    "                    gender_inputs[j,:,:] = current[0]['gender_input']\n",
    "                    inputs_dict['gender_input'] = gender_inputs\n",
    "                if self.includeTemporal:\n",
    "                    context_input_1[j,:,:] = current[0]['context_input_1']\n",
    "                    context_input_2[j,:,:] = current[0]['context_input_2']\n",
    "                    inputs_dict['context_input_1'] = context_input_1\n",
    "                    inputs_dict['context_input_2'] = context_input_2\n",
    "            # yield one batch\n",
    "            yield (inputs_dict, outputs)\n",
    "\n",
    "    # please run the splits creation notebook before....\n",
    "    def loadTrainValidTest(self):\n",
    "        with open(self.trainValidTestFN, \"rb\") as f:\n",
    "            self.trainingSet, self.validationSet, self.testSet, self.contextMap = pickle.load(f)\n",
    "            print(\"train/valid/test set size = {}/{}/{}\".format(len(self.trainingSet), len(self.validationSet), len(self.testSet)))\n",
    "            print(\"******Dataset split loaded******\") \n",
    "            \n",
    "            \n",
    "    # derive 'time_elapsed', 'distance', 'new_workout', 'derived_speed'\n",
    "    def deriveData(self, att, currentDataPoint, idx):\n",
    "        \n",
    "        if att == 'time_elapsed':\n",
    "            # Derive the time elapsed from the start\n",
    "            timestamps = currentDataPoint['timestamp']\n",
    "            initialTime = timestamps[0]\n",
    "            # Total time elapsed from the start time (better feature to use) \n",
    "            return [x - initialTime for x in timestamps]\n",
    "\n",
    "        elif att == 'distance':\n",
    "            # Derive the distance\n",
    "            lats = currentDataPoint['latitude']\n",
    "            longs = currentDataPoint['longitude']\n",
    "            indices = range(1, len(lats)) \n",
    "            distances = [0]\n",
    "            # Gets distance traveled since last time point in kilometers\n",
    "            distances.extend([haversine([lats[i-1],longs[i-1]], [lats[i],longs[i]]) for i in indices]) \n",
    "            \n",
    "            # returning a sequential feature of distance...\n",
    "            return distances\n",
    "\n",
    "        # derive the new_workout list\n",
    "        elif att == 'new_workout': \n",
    "            workoutLength = self.trimmed_workout_len\n",
    "            \n",
    "            # trimmed number of points\n",
    "            newWorkout = np.zeros(workoutLength)\n",
    "            \n",
    "            # Add the signal at start\n",
    "            newWorkout[0] = 1 \n",
    "            \n",
    "            return newWorkout\n",
    "\n",
    "        elif att == 'derived_speed':\n",
    "            # accesing the computed distance from lat-long\n",
    "            distances = self.deriveData('distance', currentDataPoint, idx)\n",
    "            timestamps = currentDataPoint['timestamp']\n",
    "            indices = range(1, len(timestamps))\n",
    "            # there is no time before 0th step\n",
    "            times = [0]\n",
    "            times.extend([timestamps[i] - timestamps[i-1] for i in indices])\n",
    "            \n",
    "            # 0 speed at 0th step\n",
    "            derivedSpeeds = [0]\n",
    "            for i in indices:\n",
    "                try:\n",
    "                    curr_speed = 3600 * distances[i] / times[i]\n",
    "                    derivedSpeeds.append(curr_speed)\n",
    "                except:\n",
    "                    # handle outlier exception cases (as speed of the previous step)\n",
    "                    derivedSpeeds.append(derivedSpeeds[i-1])\n",
    "                    \n",
    "            return derivedSpeeds\n",
    "\n",
    "        elif att == 'since_last':\n",
    "            if idx in self.contextMap:\n",
    "                total_time = self.contextMap[idx][0]\n",
    "            else:\n",
    "                # since we always drop the first workout info of each user\n",
    "                total_time = 0\n",
    "            \n",
    "            # feature multiplied with total time value\n",
    "            return np.ones(self.trimmed_workout_len) * total_time\n",
    "\n",
    "        elif att == 'since_begin':\n",
    "            if idx in self.contextMap:\n",
    "                total_time = self.contextMap[idx][1]\n",
    "            else:\n",
    "                total_time = 0\n",
    "            return np.ones(self.trimmed_workout_len) * total_time\n",
    "        else:\n",
    "            # If a random un-expected derived attribute is demanded\n",
    "            raise(Exception(\"No such derived data attribute\"))\n",
    "\n",
    "        \n",
    "    # computing z-scores and multiplying them based on a scaling paramater\n",
    "    # produces zero-centered data, which is important for the drop-in procedure\n",
    "    def scaleData(self, data, att, zMultiple=2):\n",
    "        mean, std = self.variableMeans[att], self.variableStds[att]\n",
    "        diff = [d - mean for d in data]\n",
    "        zScore = [d / std for d in diff] \n",
    "        return [x * zMultiple for x in zScore]\n",
    "\n",
    "    # perform fixed-window median smoothing on a sequence\n",
    "    def median_smoothing(self, seq, context_size):\n",
    "        # seq is a list\n",
    "        if context_size == 1: # if the window is 1, no smoothing should be applied\n",
    "            return seq\n",
    "        seq_len = len(seq)\n",
    "        if context_size % 2 == f0:\n",
    "            raise(exception(\"Context size must be odd for median smoothing\"))\n",
    "\n",
    "        smoothed_seq = []\n",
    "        # loop through sequence and smooth each position\n",
    "        for i in range(seq_len): \n",
    "            cont_diff = (context_size - 1) / 2\n",
    "            context_min = int(max(0, i-cont_diff))\n",
    "            context_max = int(min(seq_len, i+cont_diff))\n",
    "            median_val = np.median(seq[context_min:context_max])\n",
    "            smoothed_seq.append(median_val)\n",
    "\n",
    "        return smoothed_seq\n",
    "    \n",
    "    def buildEncoder(self, classLabels):\n",
    "        # Constructs a dictionary that maps each class label to a list \n",
    "        # where one entry in the list is 1 and the remainder are 0\n",
    "        encodingLength = len(classLabels)\n",
    "        encoder = {}\n",
    "        mapper = {}\n",
    "        for i, label in enumerate(classLabels):\n",
    "            encoding = [0] * encodingLength\n",
    "            encoding[i] = 1\n",
    "            encoder[label] = encoding\n",
    "            mapper[label] = i\n",
    "        return encoder, mapper\n",
    "    \n",
    "    \n",
    "    def writeSummaryFile(self):\n",
    "        metaDataForWriting=metaDataEndomondo(self.numDataPoints, self.encodingLengths, self.oneHotEncoders,  \n",
    "                                             self.oneHotMap, self.isSequence, self.isNominal, self.isDerived, \n",
    "                                             self.variableMeans, self.variableStds)\n",
    "        with open(self.metaDataFn, \"wb\") as f:\n",
    "            pickle.dump(metaDataForWriting, f)\n",
    "        print(\"Summary file written\")\n",
    "        \n",
    "    def loadSummaryFile(self):\n",
    "        try:\n",
    "            print(\"Loading metadata\")\n",
    "            with open(self.metaDataFn, \"rb\") as f:\n",
    "                metaData = pickle.load(f)\n",
    "        except:\n",
    "            raise(IOError(\"Metadata file: \" + self.metaDataFn + \" not in valid pickle format\"))\n",
    "        self.numDataPoints = metaData.numDataPoints\n",
    "        self.encodingLengths = metaData.encodingLengths\n",
    "        self.oneHotEncoders = metaData.oneHotEncoders\n",
    "        self.oneHotMap = metaData.oneHotMap\n",
    "        self.isSequence = metaData.isSequence \n",
    "        self.isNominal = metaData.isNominal\n",
    "        self.variableMeans = metaData.variableMeans\n",
    "        self.variableStds = metaData.variableStds\n",
    "        print(\"Metadata loaded\")\n",
    "\n",
    "        \n",
    "    def derive_data(self):\n",
    "        print(\"Creating Derived Data\")\n",
    "        \n",
    "        # Looping every row of the original data\n",
    "        for idx, d in tqdm(enumerate(self.original_data), position = 0,leave = True):\n",
    "            # Looping across the derived features....\n",
    "            for att in self.isDerived:\n",
    "                # add derived attribute to each row\n",
    "                self.original_data[idx][att] = self.deriveData(att, d, idx) \n",
    "            \n",
    "        \n",
    "    # Generate meta information about data\n",
    "    def buildMetaData(self):\n",
    "        if os.path.isfile(self.metaDataFn):\n",
    "            self.loadSummaryFile()\n",
    "        else:\n",
    "            # create a new file if it does not exist....\n",
    "            \n",
    "            print(\"Building data schema\")\n",
    "            # other than categoriacl, all are continuous\n",
    "            # categorical to one-hot: gender, sport\n",
    "            # categorical to embedding: userId  \n",
    "            \n",
    "            # continuous attributes\n",
    "            print(\"is sequence: {}\".format(self.isSequence))  \n",
    "            \n",
    "            # sum of variables? \n",
    "            variableSums = defaultdict(float)\n",
    "            \n",
    "            # number of categories for each categorical variable\n",
    "            classLabels = defaultdict(set)\n",
    "        \n",
    "            # consider all data to first get the max, min, etc...   \n",
    "            # Looping on entire dataset\n",
    "            for currData in self.original_data:\n",
    "                # update number of users\n",
    "                att = 'userId'\n",
    "                user = currData[att]\n",
    "                \n",
    "                # Unique users list...\n",
    "                classLabels[att].add(user)\n",
    "                \n",
    "                # update categorical attribute (adding all other nominal variables)\n",
    "                for att in self.isNominal:\n",
    "                    val  = currData[att]\n",
    "                    classLabels[att].add(val)\n",
    "                    \n",
    "                # update continuous attribute\n",
    "                for att in self.isSequence: \n",
    "                    variableSums[att] += sum(currData[att])\n",
    "\n",
    "            # One hot encoded variables for categorical features...\n",
    "            oneHotEncoders = {}\n",
    "            oneHotMap = {}\n",
    "            encodingLengths = {}\n",
    "            for att in self.isNominal:\n",
    "                oneHotEncoders[att], oneHotMap[att] = self.buildEncoder(classLabels[att]) \n",
    "                encodingLengths[att] = len(classLabels[att])\n",
    "            \n",
    "            # Creating a last one for userID\n",
    "            att = 'userId'\n",
    "            oneHotEncoders[att], oneHotMap[att] = self.buildEncoder(classLabels[att]) \n",
    "            encodingLengths[att] = 1\n",
    "            \n",
    "            # For sequential features...\n",
    "            for att in self.isSequence:\n",
    "                encodingLengths[att] = 1 # single datapoints\n",
    "            \n",
    "            # summary information\n",
    "            self.numDataPoints=len(self.original_data)\n",
    "            \n",
    "            # normalize continuous: altitude, heart_rate, latitude, longitude, speed and all derives            \n",
    "            self.computeMeanStd(variableSums, self.numDataPoints, self.isSequence)\n",
    "    \n",
    "            self.oneHotEncoders=oneHotEncoders\n",
    "            self.oneHotMap = oneHotMap\n",
    "            self.encodingLengths = encodingLengths\n",
    "            \n",
    "            #Save that summary file so that it can be used next time\n",
    "            self.writeSummaryFile()\n",
    "\n",
    " \n",
    "    def computeMeanStd(self, varSums, numDataPoints, attributes):\n",
    "        print(\"Computing variable means and standard deviations\")\n",
    "        \n",
    "        # assume each data point has 500 time step?! is it correct?\n",
    "        numSequencePoints = numDataPoints * 500 \n",
    "        \n",
    "        variableMeans = {}\n",
    "        for att in varSums:\n",
    "            variableMeans[att] = varSums[att] / numSequencePoints\n",
    "        \n",
    "        varResidualSums = defaultdict(float)\n",
    "        \n",
    "        for numDataPoints, currData in enumerate(self.original_data):\n",
    "            # loop each continuous attribute\n",
    "            for att in attributes:\n",
    "                dataPointArray = np.array(currData[att])\n",
    "                # add to the variable running sum of squared residuals\n",
    "                diff = np.subtract(dataPointArray, variableMeans[att])\n",
    "                sq = np.square(diff)\n",
    "                varResidualSums[att] += np.sum(sq)\n",
    "\n",
    "        variableStds = {}\n",
    "        for att in varResidualSums:\n",
    "            variableStds[att] = np.sqrt(varResidualSums[att] / numSequencePoints)\n",
    "            \n",
    "        self.variableMeans = variableMeans\n",
    "        self.variableStds = variableStds\n",
    "        \n",
    "        \n",
    "    # scale continuous data\n",
    "    def scale_data(self, scaling=True): \n",
    "        print(\"scale data\")\n",
    "        targetAtts = ['heart_rate', 'derived_speed']\n",
    "\n",
    "        for idx, currentDataPoint in tqdm(enumerate(self.original_data),position = 0, leave = True):\n",
    "            # target attribute, add to dict \n",
    "            for tAtt in targetAtts:         \n",
    "                if self.perform_target_smoothing:\n",
    "                    tar_data = self.median_smoothing(currentDataPoint[tAtt], self.smooth_window)\n",
    "                else:\n",
    "                    tar_data = currentDataPoint[tAtt]\n",
    "                if self.scale_targets:\n",
    "                    tar_data = self.scaleData(tar_data, tAtt, self.zMultiple) \n",
    "                self.original_data[idx][\"tar_\" + tAtt] = tar_data\n",
    "                    \n",
    "            # continuous input attribute, update dict\n",
    "            for att in self.isSequence: \n",
    "                if scaling:\n",
    "                    in_data = currentDataPoint[att]\n",
    "                    self.original_data[idx][att] = self.scaleData(in_data, att, self.zMultiple) \n",
    "        \n",
    "        for d in self.original_data:\n",
    "            key = 'url'\n",
    "            del d[key]\n",
    "            key = 'speed'\n",
    "            if key in d:\n",
    "                del d[key]\n",
    "        \n",
    "        print(\"Saving data\")\n",
    "        \n",
    "        # write to disk\n",
    "        #np.save(self.processed_path,[self.original_data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254c8ec",
   "metadata": {},
   "source": [
    "# MetaData Creator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9f73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class metaDataEndomondo(object):\n",
    "    def __init__(self, \n",
    "                 numDataPoints, \n",
    "                 encodingLengths, \n",
    "                 oneHotEncoders, \n",
    "                 oneHotMap, \n",
    "                 isSequence, \n",
    "                 isNominal, \n",
    "                 isDerived,\n",
    "                 variableMeans, \n",
    "                 variableStds):\n",
    "        \n",
    "        self.numDataPoints = numDataPoints\n",
    "        self.encodingLengths = encodingLengths\n",
    "        self.oneHotEncoders = oneHotEncoders\n",
    "        self.oneHotMap = oneHotMap\n",
    "        self.isSequence = isSequence\n",
    "        self.isNominal = isNominal\n",
    "        self.isDerived = isDerived\n",
    "        self.variableMeans = variableMeans\n",
    "        self.variableStds = variableStds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148a55b",
   "metadata": {},
   "source": [
    "# Setting up the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798a506b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input attributes:  ['distance', 'altitude', 'time_elapsed']\n",
      "target attributes:  ['tar_derived_speed']\n",
      "train/valid/test set size = 132755/16604/17058\n",
      "******Dataset split loaded******\n",
      "load original data\n",
      "Original dataset with 167783 points loaded\n",
      "Updated mapping workout id..\n",
      "##############\n",
      "Creating Derived Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167783it [04:02, 690.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new features..\n",
      "##############\n",
      "Loading metadata\n",
      "Metadata loaded\n",
      "Meta Data file generated....\n",
      "##############\n",
      "scale data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167783it [11:31, 242.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data\n",
      "Data is scaled....\n",
      "##############\n",
      "Loading metadata\n",
      "Metadata loaded\n"
     ]
    }
   ],
   "source": [
    "# Reading the dataset....\n",
    "endo_reader = dataInterpreter(inputAtts = inputAtts,\n",
    "                              trainValidTestFN = trainValidTestFN)\n",
    "    \n",
    "endo_reader.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c30890",
   "metadata": {},
   "source": [
    "# Embeddings creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e67f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import keras\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, concatenate, Reshape, Concatenate, Flatten, Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.merge import Concatenate, Add, Dot, concatenate, add, dot, multiply\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from keras.regularizers import l2\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "import sys, argparse\n",
    "import pandas as pd\n",
    "#from data_interpreter_Keras_aux import dataInterpreter, metaDataEndomondo\n",
    "import pickle\n",
    "from math import floor\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10cd96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining args\n",
    "def parse():\n",
    "    parser = argparse.ArgumentParser(description='context2seq-NAT')\n",
    "    parser.add_argument('--patience', default=10, type=int, help='patience for early stop') # [3,5,10,20]\n",
    "    parser.add_argument('--epoch', default=50, type=int, help='max epoch') # [50,100]\n",
    "    parser.add_argument('--attributes', default=\"userId,sport,gender\", help='input attributes')\n",
    "    parser.add_argument('--input_attributes', default=\"distance,altitude,time_elapsed\", help='input attributes')\n",
    "    parser.add_argument('--pretrain', action='store_true', help='use pretrain model')\n",
    "    parser.add_argument('--temporal', action='store_true', help='use temporal input')\n",
    "    parser.add_argument('--batch_size', default=256, type=int, help='batch size') # \n",
    "    parser.add_argument('--attr_dim', default=5, type=int, help='attribute dimension') # \n",
    "    parser.add_argument('--hidden_dim', default=64, type=int, help='rnn hidden dimension') # \n",
    "    parser.add_argument('--lr', default=0.005, type=float, help='learning rate') # 0.001 for fine tune; 0.005 for general\n",
    "    parser.add_argument('--user_reg', default=0.0, type=float, help='user attribute reg') \n",
    "    parser.add_argument('--sport_reg', default=0.01, type=float, help='sport attribute reg') \n",
    "    parser.add_argument('--gender_reg', default=0.05, type=float, help='gender attribute reg') \n",
    "    parser.add_argument('--out_reg', default=0.0, type=float, help='final output layer reg') \n",
    "    parser.add_argument('--pretrain_file', default=\"\", help='pretrain file') \n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c94ad0",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef44feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da7fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_holder(object):\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser(description='context2seq-NAT')\n",
    "        \n",
    "        self.patience = 10\n",
    "        self.epoch = 50\n",
    "        self.attributes = \"userId,sport,gender\"\n",
    "        self.input_attributes = \"distance,altitude,time_elapsed\"\n",
    "        self.pretrain = True\n",
    "        self.temporal = True\n",
    "        self.batch_size = 256\n",
    "        self.attr_dim = 5\n",
    "        self.hidden_dim = 64\n",
    "        self.lr = 0.005\n",
    "        self.user_reg = 0.0\n",
    "        self.sport_reg = 0.01\n",
    "        self.gender_reg = 0.05\n",
    "        self.out_reg = 0.0\n",
    "        self.pretrain_file = \"\"\n",
    "        \n",
    "args = args_holder()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b02131",
   "metadata": {},
   "source": [
    "# Main LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5605af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class keras_endoLSTM(object):\n",
    "    def __init__(self, args, newModel):\n",
    "        \n",
    "        # Model training from scratch....\n",
    "        if newModel:\n",
    "            \n",
    "            # Logging & save directory.....\n",
    "            self.model_save_location = \"./fitrec/model_states/\"\n",
    "            self.summaries_dir = path + \"./fitrec/logs/\"\n",
    "            self.data_path = \"endomondoHR_proper.json\"\n",
    "            self.trainValidTestFN = self.data_path.split(\".\")[0] + \"_temporal_dataset.pkl\"\n",
    "            \n",
    "            # Training details......\n",
    "            self.patience = args.patience # [3,5,10]\n",
    "            self.max_epochs = args.epoch # [50,100]\n",
    "            print(\"patience={}\".format(self.patience))\n",
    "            print(\"max_epochs={}\".format(self.max_epochs))\n",
    "            \n",
    "            # Normalising properties & other hyper-parameters...\n",
    "            self.zMultiple = 5\n",
    "            self.attrFeatures = args.attributes.split(',')\n",
    "            self.user_dim = args.attribute_dim\n",
    "            self.sport_dim = args.attribute_dim\n",
    "            self.gender_dim = args.attribute_dim\n",
    "            self.includeUser = 'userId' in self.attrFeatures\n",
    "            self.includeSport = 'sport' in self.attrFeatures\n",
    "            self.includeGender = 'gender' in self.attrFeatures\n",
    "\n",
    "            self.pretrain = args.pretrain\n",
    "            self.includeTemporal = args.temporal\n",
    "            self.pretrain_model_file_name = args.pretrain_file\n",
    "\n",
    "            self.lr = args.lr\n",
    "            print(\"RMSprop lr = {}\".format(self.lr))\n",
    "            \n",
    "            # status of feature inclusion\n",
    "            print(\"include pretrain/user/sport/gender/temporal = {}/{}/{}/{}/{}\".format(self.pretrain, \n",
    "                                                                                        self.includeUser, \n",
    "                                                                                        self.includeSport, \n",
    "                                                                                        self.includeGender, \n",
    "                                                                                        self.includeTemporal))\n",
    "\n",
    "            self.model_file_name = []\n",
    "            self.model_file_name.extend(self.attrFeatures)\n",
    "            if self.includeTemporal:\n",
    "                self.model_file_name.append(\"context\")\n",
    "            print(self.model_file_name)\n",
    "\n",
    "            self.trainValidTestSplit = [0.8, 0.1, 0.1]\n",
    "            self.targetAtts = ['heart_rate']\n",
    "            self.inputAtts = args.input_attributes\n",
    "            \n",
    "            self.trimmed_workout_len = 450\n",
    "            self.num_steps = self.trimmed_workout_len\n",
    "            self.batch_size_m = args.batch_size\n",
    "            # Should the data values be scaled to their z-scores with the z-multiple?\n",
    "            self.scale = True\n",
    "            self.scaleTargets = False \n",
    "\n",
    "            self.endo_reader = dataInterpreter(self.inputAtts, \n",
    "                                               self.targetAtts, \n",
    "                                               self.includeUser,\n",
    "                                               self.includeSport, \n",
    "                                               self.includeGender,\n",
    "                                               self.includeTemporal,  \n",
    "                                               fn=self.data_path, \n",
    "                                               scaleVals=self.scale,\n",
    "                                               trimmed_workout_len=self.trimmed_workout_len, \n",
    "                                               scaleTargets=self.scaleTargets, \n",
    "                                               trainValidTestSplit=self.trainValidTestSplit, \n",
    "                                               zMultiple = self.zMultiple, \n",
    "                                               trainValidTestFN=self.trainValidTestFN)\n",
    "\n",
    "            # preprocess data: scale\n",
    "            self.endo_reader.preprocess_data()\n",
    "            self.input_dim = self.endo_reader.input_dim \n",
    "            self.output_dim = self.endo_reader.output_dim \n",
    "            self.train_size = len(self.endo_reader.trainingSet)\n",
    "            self.valid_size = len(self.endo_reader.validationSet)\n",
    "            self.test_size = len(self.endo_reader.testSet)\n",
    "            # build model\n",
    "            self.model = self.build_model(args)\n",
    "\n",
    "\n",
    "    def build_model(self, args):\n",
    "        print('Build model...')\n",
    "        self.num_users = len(self.endo_reader.oneHotMap['userId'])\n",
    "        self.num_sports = len(self.endo_reader.oneHotMap['sport'])\n",
    "\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        user_reg = args.user_reg\n",
    "        sport_reg = args.sport_reg\n",
    "        gender_reg = args.gender_reg\n",
    "        output_reg = args.output_reg\n",
    "        print(\"user/sport/output regularizer = {}/{}/{}\".format(user_reg, sport_reg, output_reg))\n",
    "        \n",
    "        # Embedding layer...\n",
    "        inputs = Input(shape=(self.num_steps,self.input_dim), name='input')\n",
    "        self.layer1_dim = self.input_dim\n",
    "\n",
    "        if self.includeUser:\n",
    "            user_inputs = Input(shape=(self.num_steps,1), name='user_input')\n",
    "            User_Embedding = Embedding(input_dim=self.num_users, output_dim=self.user_dim, name='user_embedding', \n",
    "                                       embeddings_initializer=initializers.random_normal(stddev=0.01), embeddings_regularizer=l2(user_reg))\n",
    "            user_embedding = User_Embedding(user_inputs)\n",
    "            user_embedding = Lambda(lambda y: K.squeeze(y, 2))(user_embedding) \n",
    "            self.layer1_dim += self.user_dim\n",
    "\n",
    "        if self.includeSport:\n",
    "            sport_inputs = Input(shape=(self.num_steps,1), name='sport_input')\n",
    "            Sport_Embedding = Embedding(input_dim=self.num_sports, output_dim=self.sport_dim, name='sport_embedding', \n",
    "                                   embeddings_initializer=initializers.random_normal(stddev=0.01), embeddings_regularizer=l2(sport_reg))\n",
    "            sport_embedding = Sport_Embedding(sport_inputs)\n",
    "            sport_embedding = Lambda(lambda y: K.squeeze(y, 2))(sport_embedding) \n",
    "            self.layer1_dim += self.sport_dim\n",
    "\n",
    "        if self.includeGender:\n",
    "            gender_inputs = Input(shape=(self.num_steps,1), name='gender_input')\n",
    "            Gender_Embedding = Embedding(input_dim=self.num_users, output_dim=self.gender_dim, name='gender_embedding', \n",
    "                                       embeddings_initializer=initializers.random_normal(stddev=0.01), embeddings_regularizer=l2(gender_reg))\n",
    "            gender_embedding = Gender_Embedding(gender_inputs)\n",
    "            gender_embedding = Lambda(lambda y: K.squeeze(y, 2))(gender_embedding) \n",
    "            self.layer1_dim += self.gender_dim\n",
    "\n",
    "        if self.includeTemporal:\n",
    "            context_input_1 = Input(shape=(self.num_steps,self.input_dim + 1), name='context_input_1') # add 1 for since_last\n",
    "            context_input_2 = Input(shape=(self.num_steps,self.output_dim), name='context_input_2')\n",
    "\n",
    "        predict_vector = inputs\n",
    "        if self.includeUser:\n",
    "            predict_vector = concatenate([predict_vector, user_embedding])\n",
    "\n",
    "        if self.includeSport:\n",
    "            predict_vector = concatenate([predict_vector, sport_embedding])\n",
    "\n",
    "        if self.includeGender:\n",
    "            predict_vector = concatenate([predict_vector, gender_embedding]) \n",
    "\n",
    "        if self.includeTemporal:\n",
    "            self.context_dim = self.hidden_dim \n",
    "            context_layer_1 = LSTM(self.hidden_dim, return_sequences=True, input_shape=(self.num_steps, self.input_dim), name='context_layer_1')\n",
    "            context_layer_2 = LSTM(self.hidden_dim, return_sequences=True, input_shape=(self.num_steps, self.output_dim), name='context_layer_2')\n",
    "            context_embedding_1 = context_layer_1(context_input_1)\n",
    "            context_embedding_2 = context_layer_2(context_input_2)\n",
    "            context_embedding = concatenate([context_embedding_1, context_embedding_2])\n",
    "            context_embedding = Dense(self.context_dim, activation='selu', name='context_projection')(context_embedding)\n",
    "            predict_vector = concatenate([context_embedding, predict_vector]) \n",
    "            self.layer1_dim += self.context_dim\n",
    "\n",
    "        \n",
    "        # Main prediction moodel....\n",
    "        layer1 = LSTM(self.hidden_dim, return_sequences=True, input_shape=(self.num_steps, self.layer1_dim), name='layer1')(predict_vector)\n",
    "        dropout1 = Dropout(0.2, name='dropout1')(layer1)\n",
    "        layer2 = LSTM(self.hidden_dim, return_sequences=True, name='layer2')(dropout1)\n",
    "        dropout2 = Dropout(0.2, name='dropout2')(layer2)\n",
    "        output = Dense(self.output_dim, name='output', kernel_regularizer=l2(output_reg))(dropout2)\n",
    "        predict = Activation('selu', name='selu_activation')(output)\n",
    "        #predict = Activation('linear', name='linear_activation')(output)\n",
    "\n",
    "        inputs_array = [inputs]\n",
    "        if self.includeUser:\n",
    "            inputs_array.append(user_inputs)\n",
    "        if self.includeSport:\n",
    "            inputs_array.append(sport_inputs)\n",
    "        if self.includeGender:\n",
    "            inputs_array.append(gender_inputs)\n",
    "        if self.includeTemporal:\n",
    "            inputs_array.extend([context_input_1, context_input_2])\n",
    "        model = Model(inputs=inputs_array, outputs=[predict])\n",
    "\n",
    "        # compile model\n",
    "        model.compile(loss='mean_squared_error', optimizer=RMSprop(lr=self.lr), metrics=['mae', root_mean_squared_error])\n",
    "\n",
    "        print(\"Endomodel Built!\")\n",
    "        model.summary()\n",
    "\n",
    "        if self.pretrain == True:\n",
    "            print(\"pretrain model: {}\".format(self.pretrain_model_file_name))\n",
    "            filepath = \"./\"+self.pretrain_model_file_name+\"_bestValidScore\"\n",
    "\n",
    "            custom_ob = {'root_mean_squared_error':root_mean_squared_error}\n",
    "            pretrain_model = keras.models.load_model(self.model_save_location+self.pretrain_model_file_name+\"/\"+self.pretrain_model_file_name+\"_bestValidScore\", custom_objects=custom_ob) \n",
    "\n",
    "            layer_dict = dict([(layer.name, layer) for layer in pretrain_model.layers])\n",
    "            for layer_name in layer_dict:\n",
    "                weights = layer_dict[layer_name].get_weights()\n",
    "    \n",
    "                if layer_name=='layer1':\n",
    "                    weights[0] = np.vstack([weights[0],\n",
    "                                            np.zeros((self.layer1_dim - self.input_dim - self.user_dim - self.sport_dim, self.hidden_dim * 4)).astype(np.float32)])\n",
    "\n",
    "                model.get_layer(layer_name).set_weights(weights)\n",
    "            del pretrain_model\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def run_model(self, model):\n",
    "\n",
    "        modelRunIdentifier = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        self.model_file_name.append(modelRunIdentifier) # Applend a unique identifier to the filenames\n",
    "        self.model_file_name = \"_\".join(self.model_file_name)\n",
    "\n",
    "        self.model_save_location += self.model_file_name + \"/\"\n",
    "        self.summaries_dir += self.model_file_name + \"/\"\n",
    "        os.mkdir(self.model_save_location) \n",
    "        os.mkdir(self.summaries_dir) \n",
    "\n",
    "        best_valid_score = 9999999999\n",
    "        best_epoch = 0\n",
    "      \n",
    "        train_steps_per_epoch = int(self.train_size * self.trimmed_workout_len / (self.num_steps * self.batch_size_m))\n",
    "        valid_steps_per_epoch = int(self.valid_size * self.trimmed_workout_len / (self.num_steps * self.batch_size_m))\n",
    "        test_steps_per_epoch = int(self.test_size * self.trimmed_workout_len / (self.num_steps * self.batch_size_m))\n",
    "\n",
    "        # avoid process data in each iterator?\n",
    "        for iteration in range(1, self.max_epochs):\n",
    "            print()\n",
    "            print('-' * 50)\n",
    "            print('Iteration', iteration)\n",
    "\n",
    "            trainDataGen = self.endo_reader.generator_for_autotrain(self.batch_size_m, self.num_steps, \"train\")  \n",
    "            \n",
    "            model_save_fn = self.model_save_location + self.model_file_name + \"_epoch_\"+str(iteration)\n",
    "            checkpoint = ModelCheckpoint(model_save_fn, verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='auto')\n",
    "\n",
    "            history = model.fit_generator(trainDataGen, train_steps_per_epoch, epochs=1, verbose=1, callbacks=[checkpoint])\n",
    "            try:\n",
    "                del history.model\n",
    "                with open(self.summaries_dir+\"model_history_\"+self.model_file_name+\"_epoch_\"+str(iteration), \"wb\") as f:\n",
    "                    pickle.dump(history, f)\n",
    "                print(\"Model history saved\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            validDataGen = self.endo_reader.generator_for_autotrain(self.batch_size_m, self.num_steps, \"valid\")\n",
    "            valid_score = model.evaluate_generator(validDataGen, valid_steps_per_epoch)\n",
    "            print(\"Valid score: \", valid_score)\n",
    "            try:\n",
    "                with open(self.summaries_dir+\"model_valid_score_\"+self.model_file_name+\"_epoch_\"+str(iteration), \"wb\") as f:\n",
    "                    pickle.dump(valid_score, f)\n",
    "                print(\"Model validation score saved\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if valid_score[0] <= best_valid_score:\n",
    "                best_valid_score = valid_score[0]\n",
    "                best_epoch = iteration\n",
    "            elif (iteration-best_epoch < self.patience):\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Stopped early at epoch: \" + str(iteration))\n",
    "                break\n",
    "\n",
    "        # load best model\n",
    "        custom_ob = {'root_mean_squared_error':root_mean_squared_error}\n",
    "        best_model = keras.models.load_model(self.model_save_location+self.model_file_name+\"_epoch_\"+str(best_epoch), custom_objects=custom_ob)\n",
    "        best_model.save(self.model_save_location+self.model_file_name+\"_bestValidScore\")\n",
    "\n",
    "        print(\"Best epoch: \" + str(best_epoch) + \" validation score: \" + str(best_valid_score))\n",
    "\n",
    "        print(\"Testing\")\n",
    "        testDataGen = self.endo_reader.generator_for_autotrain(self.batch_size_m, self.num_steps, \"test\")\n",
    "        test_score = best_model.evaluate_generator(testDataGen, test_steps_per_epoch)\n",
    "        print(\"Test score: \" + str(test_score))\n",
    "        print(\"Done!!!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
